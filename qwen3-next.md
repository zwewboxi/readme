# 25.12.16汇报 Qwen3-Next 模型架构

## 1. 整体架构
- **总层数**: 48层 = 12 × (3层Gated DeltaNet + 1层Gated Attention)的重复结构
- **隐藏维度**: 2048

## 2. Gated DeltaNet 结构

### 2.1 结构信息
| 组件 | 头数 | 头维度 | 总维度 |
|------|------|--------|--------|
| Q/K | 16 | 128 | 2048 (16×128) |
| V   | 32 | 128 | 4096 (32×128) |

### 2.2 线性投影
**输入**: `X` (形状: `[B, L, 2048]`)

**投影矩阵**: 
- `W_qkvz` (形状: `[2048, 12288]`)
- `12288 = k_dim(2048) + v_dim(4096) + z_dim(4096) + q_dim(2048)`
- `W_ba` (形状: `[2048, 64]`)

**投影操作**: 
```python
X @ W_qkvz = [q, k, v, z]  # 总维度 12288
X @ W_ba = [b, a]  # 总维度 64
```

**维度分解**:
- `q`: `[B, L, 32, 128]` (query)
- `k`: `[B, L, 16, 128]` (key)
- `v`: `[B, L, 16, 128]` (value)
- `z`: `[B, L, 32, 128]` (门控参数，与qkv投影一起得到)
- `b`: `[B, L, 32]` (β参数的原始值)
- `a`: `[B, L, 32]` (衰减函数的原始值)

### 2.3 卷积变换
对特征通道进行卷积变换：
- `q`: `[B, L, 32, 128]` → `[B, L, 4096]`
- `k`: `[B, L, 16, 128]` → `[B, L, 2048]`
- `v`: `[B, L, 16, 128]` → `[B, L, 2048]`

### 2.4 激活函数
使用 **SiLU激活函数**

### 2.5 Gated Delta Rule 线性注意力

#### 步骤1: 重复q和k头以匹配v头数
```python
Q_repeat = repeat(q, [1, 1, 2, 1])  # [B, L, 32, 128]
K_repeat = repeat(k, [1, 1, 2, 1])  # [B, L, 32, 128]
```

#### 步骤2: DeltaNet状态更新（递归形式）
```python
# 初始化状态
S = zeros(B, 32, 128, 128)  # [B, 32, 128, 128]

# 计算β和g
β = torch.sigmoid(b) # (B, L, 32)
g = -self.A_log.float().exp() * F.softplus(a.float() + self.dt_bias) # (B, L, 32)

for t = 1 to L:
    # 获取当前时间步的各个组件
    Q_t = Q_repeat[:, t, :, :]  # [B, 32, 128]
    K_t = K_repeat[:, t, :, :]  # [B, 32, 128]
    V_t = V[:, t, :, :]         # [B, 32, 128]
    β_t = β[:, t, :]            # [B, 32]
    g_t = exp(g[:, t, :])       # [B, 32]
    
    # 扩展维度用于广播
    g_t_exp = expand(g_t, [-1, -1, 128, 128])  # [B, 32, 128, 128]
    β_t_exp = expand(β_t, [-1, -1, 128])       # [B, 32, 128]
    
    # 状态更新
    S = S * g_t_exp                     # 衰减
    ΔS = K_t^T @ (V_t * β_t_exp)        # 外积更新
    S = S + ΔS
    
    # 计算输出
    O_t = Q_t @ S  # [B, 32, 128]
```

#### 步骤3: 合并所有时间步
```python
O = stack([O_t for t in 1..L], dim=1)  # [B, L, 32, 128]
```

### 对比标准注意力方法
- `标准注意力`  ：O(B*L^2*d), KV缓存
- `Gated DeltaNet线性注意力` ：O(B*L*d^2), 存储状态S

## 3. Gated Attention 结构
> 进行补充
