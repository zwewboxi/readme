
##  **ASR 模型参数规模汇总**

| 模型名称 | 参数量（约） | 发布机构 | 备注 |
|----------|--------------|----------|------|
| **Fun-ASR** | 7.7B | 阿里 | 音频编码器0.7B + LLM解码器7B |
| **Fun-ASR-nano** | 0.8B | 阿里 | 轻量版，音频编码器0.2B + LLM解码器0.6B |
| **Whisper-large-v3** | 1.5B | OpenAI | 多语言，弱监督训练 |
| **Seed-ASR** | ~12B | 未明确（论文中提及） | 基于LLM的长上下文ASR |
| **FireRed-ASR** | 1.1B | 未明确（论文中提及） | 工业级中文ASR，开源 |
| **Kimi-Audio** | 8B | KimiTeam | 语音-文本多模态模型 |
| **Paraformer-v2** | 未明确 | 阿里 | 非自回归，噪声鲁棒 |
| **Step-Audio2** | 未明确 | 未明确 | 未提供详细信息 |
| **Fun-ASR-ML** | 未明确（同Fun-ASR规模） | 阿里 | 多语言版本（中、英、越南、泰、印尼） |
| **dolphin-small** | 未明确 | 未明确 | 东亚语言ASR模型 |
| **seamless-m4t-large-v2** | 未明确 | Meta | 多语言语音翻译与识别 |

---

##  **各模型在开源数据集上的 WER%**

| 测试集 | Fun-ASR | Fun-ASR-nano | Whisper-large-v3 | Seed-ASR* | FireRed-ASR | Kimi-Audio | Step-Audio2 |
|--------|----------|---------------|------------------|-----------|-------------|------------|-------------|
| AIShell1 | 1.22 | 1.76 | 4.72 | 1.63 | 0.54 | 0.71 | 0.63 |
| AIShell2 | 2.30 | 2.80 | 4.68 | 2.76 | 2.58 | 2.86 | 2.10 |
| Fleurs-zh | 2.64 | 3.47 | 5.18 | 3.23 | 4.81 | 3.11 | 2.68 |
| Fleurs-en | 5.84 | 7.95 | 6.23 | 9.39 | 10.79 | 6.99 | 3.03 |
| Librispeech-clean | 1.57 | 1.75 | 1.86 | 2.80 | 1.84 | 1.32 | 1.17 |
| Librispeech-other | 3.24 | 4.37 | 3.43 | 5.69 | 4.52 | 2.63 | 2.42 |
| WenetSpeech Meeting | 6.49 | 8.78 | 18.39 | 7.07 | 4.95 | 6.24 | 4.75 |
| WenetSpeech Net | 5.46 | 6.28 | 11.89 | 4.84 | 4.94 | 6.45 | 4.67 |

> **注：** Seed-ASR* 是通过其官方API评估的结果。

---

##  **各模型在工业数据集上的 WER%（表2数据）**

| 测试集 | Fun-ASR | Fun-ASR-nano | Seed-ASR | Whisper-large-v3 | FireRed-ASR | Kimi-Audio | Paraformer v2 |
|--------|----------|---------------|-----------|------------------|-------------|------------|---------------|
| In-house | 6.66 | 7.26 | 7.20 | 16.58 | 10.10 | 9.02 | 8.11 |
| Fairfield | 4.66 | 5.43 | 4.59 | 22.21 | 7.49 | 10.95 | 9.55 |
| Home Scenario | 5.17 | 6.02 | 8.08 | 18.17 | 9.67 | 23.79 | 6.87 |
| Complex Background | 11.29 | 17.07 | 12.90 | 32.57 | 15.56 | 15.56 | 15.19 |
| English General | 14.22 | 15.87 | 15.65 | 18.56 | 21.62 | 18.12 | 19.48 |
| Opensource | 3.60 | 4.65 | 3.83 | 7.05 | 5.31 | 3.20 | 6.23 |
| **Average** | **7.60** | **9.38** | **8.71** | **19.19** | **11.63** | **13.54** | **10.91** |

---

##  **多语言 ASR 模型对比（WER%）**

| 语言 | 测试集 | Fun-ASR-ML | Kimi-Audio | Whisper-large-v3 | dolphin-small | seamless-m4t-large-v2 |
|------|--------|-------------|------------|------------------|---------------|-----------------------|
| 中文 | fleurs | 3.00 | 2.69 | 4.71 | 5.46 | 5.15 |
| 中文 | commonvoice | 5.76 | 7.21 | 12.61 | 9.94 | 10.76 |
| 英文 | fleurs | 3.18 | 4.40 | 4.11 | N.A. | 6.59 |
| 英文 | commonvoice | 1.67 | 10.31 | 9.66 | N.A. | 7.62 |
| 印尼语 | fleurs | 8.10 | N.A. | 6.07 | 15.86 | 9.36 |
| 泰语 | fleurs | 6.00 | N.A. | 8.48 | 9.66 | 9.25 |
| 越南语 | fleurs | 5.50 | N.A. | 6.51 | 15.62 | 8.07 |

---

##  **专项能力对比**

### **1. 流式 ASR**
| 测试集 | Fun-ASR（流式） | Fun-ASR-nano（流式） | Seed-ASR（流式） |
|--------|-----------------|---------------------|-----------------|
| In-house | 7.00 | 7.97 | 8.64 |
| Fairfield | 5.33 | 6.92 | 5.51 |
| Home Scenario | 5.33 | 6.51 | 9.70 |
| Complex Background | 12.50 | 14.83 | 15.48 |

### **2. 噪声鲁棒性（Fun-ASR 自身对比）**
| 环境 | 无噪声训练 | 有噪声训练 | 噪声训练+RL |
|------|------------|------------|-------------|
| dinner | 14.02 | 9.88 | 9.55 |
| supermarket | 14.27 | 8.81 | 8.75 |
| 平均 | 13.32 | 11.58 | 11.45 |

### **3. 中英混说**
| 测试集 | 无混说训练 | 有混说训练（无RL） | 有混说训练（有RL） |
|--------|------------|-------------------|-------------------|
| A | 4.53 | 1.70 | 1.59 |
| B | 4.76 | 4.56 | 4.50 |

### **4. 热词定制效果（部分示例）**
| 主题 | 无RL的准确率/召回率 | 有RL的准确率/召回率 |
|------|-------------------|-------------------|
| 人名 | 1.00 / 0.95 | 1.00 / 1.00 |
| 生物 | 0.98 / 0.99 | 0.97 / 1.00 |
| 哲学 | 0.99 / 0.96 | 0.99 / 0.97 |

---

##  **总结对比**

- **参数规模**：Seed-ASR 最大（~12B），Fun-ASR 居中（7.7B），FireRed-ASR 和 Whisper 较小（1.1B–1.5B）。
- **精度表现**：
  - 开源数据集上，FireRed-ASR 和 Kimi-Audio 在某些中文任务上略优。
  - 工业数据集上，Fun-ASR 在多数场景下表现最佳，尤其在噪声、复杂背景下。
- **多语言**：Fun-ASR-ML 在东南亚语言上表现优于其他多语言模型。
- **实用能力**：Fun-ASR 在流式、噪声鲁棒、混说、热词定制等方面均有专门优化，适合工业部署。

---
