---

# 大模型对齐算法深度解析：PPO vs DPO vs GRPO

## 一、 算法深度拆解

### 1. PPO (Proximal Policy Optimization)
**—— 经典的“三步走”策略**

#### A. 原理与流程
PPO 是强化学习中最经典的**策略梯度算法**，也是早期大模型（如 ChatGPT）对齐的标配。它遵循标准的 **RLHF（基于人类反馈的强化学习）** 流程：
1.  **SFT (有监督微调)**：先用标注数据训练一个初始模型。
2.  **RM (奖励模型训练)**：让人类对模型的输出进行打分，训练一个模型（RM）来模拟人类的打分标准。
3.  **RL (强化学习)**：让 SFT 模型与环境交互，根据 RM 给出的分数（奖励）不断调整参数。
4.  **核心机制**：为了防止模型更新步子迈太大导致崩溃（训练不稳定），PPO 引入了“截断（Clip）”机制，强制限制新旧策略的差异。

#### B. 数学公式
PPO 的目标函数由三部分组成：**策略损失**、**价值损失**和**熵奖励**。

$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right] - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta](s_t)
$$

#### C. 参数含义
*   $r_t(\theta)$：**概率比率**。$\frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)}$，表示新策略选择该动作的概率相对于旧策略的变化倍数。
*   $A_t$：**优势函数 (Advantage)**。$Q(s_t, a_t) - V(s_t)$，表示采取动作 $a_t$ 比平均水平好多少。
*   $\epsilon$：**截断系数**。通常设为 0.2，限制 $r_t$ 在 [0.8, 1.2] 之间，防止更新幅度过大。
*   $\text{min}(\dots)$：**截断操作**。如果 $r_t A_t$ 过大（更新过猛），就将其截断到 $1+\epsilon$ 处。
*   $L^{VF}$：**价值函数损失**。预测状态价值的误差。
*   $S[\pi_\theta]$：**熵 (Entropy)**。鼓励模型输出的多样性，防止模型“偷懒”只输出固定答案。

#### D. 目的与结果
*   **目的**：在 RM 的指导下，最大化模型的累计奖励，同时保证训练过程的稳定性。
*   **优点**：
    *   理论成熟，收敛性好。
    *   对齐效果显著，是闭源模型的标杆。
*   **缺点**：
    *   **流程繁琐**：需要训练 SFT、RM，最后还要跑复杂的 RL 循环。
    *   **过拟合风险**：RM 容易过拟合人类标注数据，且 RL 阶段容易导致模型遗忘预训练知识（灾难性遗忘）。
    *   **训练成本极高**：需要大量的计算资源进行采样和迭代。

---

### 2. DPO (Direct Preference Optimization)
**—— 极简的“直接优化”策略**

#### A. 原理与流程
DPO 是对 RLHF 的重大简化，它**跳过了奖励模型（RM）和强化学习（RL）的循环**。
1.  **数据准备**：只需要**偏好数据**（即：对于同一个问题 $x$，回答 $y_w$ 比 $y_l$ 好）。
2.  **直接优化**：利用贝叶斯定理，将“偏好数据”直接转化为对策略模型的约束。
3.  **核心机制**：通过调整模型参数，使得模型生成“好回答”的概率与“坏回答”的概率之比，符合人类的偏好程度。同时引入 KL 散度约束，防止模型偏离初始的 SFT 模型太远。

#### B. 数学公式
DPO 的目标是最小化以下损失函数：

$$
L_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)} - \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} \right) \right) \right]
$$

或者等价地看作最大化以下目标（在 KL 约束下）：
$$
\max_\theta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_w|x) + \pi_\theta(y_l|x)} \right] - \frac{1}{\beta} D_{KL}(\pi_\theta(\cdot|x) \| \pi_{ref}(\cdot|x))
$$

#### C. 参数含义
*   $\pi_\theta$：**当前待优化的策略模型**。
*   $\pi_{ref}$：**参考模型**。通常是 SFT 模型，作为基准线。
*   $y_w, y_l$：**赢者（好回答）**和**输者（坏回答）**。
*   $\beta$：**温度系数**。控制对齐的强度。$\beta$ 越大，模型越想把好回答和坏回答区分开（可能导致输出过于单一）。
*   $\sigma$：**Sigmoid 函数**。将概率差映射到 0-1 之间，作为分类损失。
*   $D_{KL}$：**KL 散度**。衡量两个分布的距离，防止模型“学歪”。

#### D. 目的与结果
*   **目的**：在不训练 RM 的情况下，直接利用偏好数据对齐模型，同时保持训练的稳定性。
*   **优点**：
    *   **极大简化流程**：省去了训练 RM 和复杂的 RL 采样过程，训练速度快。
    *   **数学优美**：理论上证明了 DPO 的解等价于 RLHF 的最优解。
    *   **鲁棒性强**：不容易过拟合，对超参数不敏感。
*   **缺点**：
    *   **对数据质量要求高**：因为没有 RM 作为缓冲，偏好数据中的噪声会直接影响模型。
    *   **缺乏探索**：相比 RL，DPO 更像是在做监督学习，缺乏主动探索环境的能力。

---

### 3. GRPO (Guided Reference Policy Optimization)
**—— 进阶的“混合引导”策略**

#### A. 原理与流程
GRPO 是为了解决 PPO 和 DPO 的各自痛点而提出的**混合算法**。它结合了 PPO 的“奖励引导”和 DPO 的“偏好直接优化”。
1.  **数据融合**：它可以同时利用**奖励分数**（来自 RM）和**偏好对**数据。
2.  **动态参考**：引入了一个**“引导参考策略” ($\pi_g$)**，这个策略不是固定的 SFT 模型，而是根据当前训练进度动态调整的（通常是旧策略和当前策略的混合）。
3.  **核心机制**：在优化目标中，既包含了类似 DPO 的偏好相对优势，又包含了类似 PPO 的近端约束（Clip），旨在利用动态参考策略引导模型更平滑地收敛到最优解。

#### B. 数学公式
GRPO 的目标函数较为复杂，融合了偏好项和奖励项：

$$
\begin{aligned}
L^{GRPO}(\theta) &= \mathbb{E}_{(x,y_1,y_2)\sim \mathcal{D}_{pref}} \left[ \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_1|x)}{\pi_\theta(y_2|x)} \right) \right) \right] \\
&+ \mathbb{E}_{(s,a)\sim \mathcal{D}_{rl}} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] \\
&- \lambda D_{KL}(\pi_\theta \| \pi_g)
\end{aligned}
$$

*(注：不同论文实现细节略有差异，但核心都是“偏好 + 奖励 + 动态 KL/Clip”)*

#### C. 参数含义
*   $\pi_g$：**引导参考策略**。这是 GRPO 的核心创新，用于替代 PPO 中的旧策略或 DPO 中的固定参考策略，提供更灵活的约束。
*   $A_t$：**优势函数**。基于奖励模型计算。
*   其他参数：与 PPO 和 DPO 含义一致。

#### D. 目的与结果
*   **目的**：结合 PPO 的高样本效率（利用奖励）和 DPO 的训练稳定性（利用偏好），通过动态参考策略解决“过矫正”问题。
*   **优点**：
    *   **兼容性强**：可以无缝接入现有的 RLHF 流水线。
    *   **收敛更平滑**：动态参考策略能有效防止模型在训练后期震荡。
    *   **效果提升**：在复杂任务（如长文本生成、多模态）上往往优于单一方法。
*   **缺点**：
    *   **实现复杂**：需要维护多个模型（策略、价值、参考、奖励），工程落地难度大。
    *   **调参困难**：超参数众多，需要大量经验才能调好。

---

## 二、 三种方法的横向对比总结

| 维度 | PPO (近端策略优化) | DPO (直接偏好优化) | GRPO (引导参考策略优化) |
|:--- |:--- |:--- |:--- |
| **核心范式** | 传统 RLHF (SFT → RM → RL) | 简化 RLHF (SFT → 直接优化) | 混合增强 (偏好 + 奖励 + 动态参考) |
| **是否需要 RM** | **必须** | **不需要** | **可选/推荐** |
| **数据依赖** | 奖励分数 (标量) | 偏好对 (相对比较) | 偏好对 + 奖励分数 |
| **约束机制** | 硬性截断 (Clip) + 熵 | KL 散度 (相对熵) | 动态 KL + Clip |
| **训练速度** | 慢 (需多轮采样迭代) | 快 (单轮梯度下降) | 中等 |
| **稳定性** | 中等 (易震荡) | 高 (理论保证) | 高 (动态引导) |
| **实现难度** | 高 (涉及 Actor-Critic 架构) | 低 (类似监督学习) | 极高 (多模型联动) |
| **适用场景** | 算力充足、追求极致效果的闭源模型 | 算力有限、追求快速迭代的开源模型 | 复杂任务、需要精细控制的工业级模型 |

---

## 三、 主流大模型的应用与发展趋势

### 1. 早期阶段：PPO 的统治
在 2022-2023 年初，**PPO** 是绝对的王者。
*   **代表模型**：**ChatGPT (GPT-3.5/4)**、**Claude 1**、**LLaMA-2 (早期版本)**。
*   **发展状态**：OpenAI 凭借强大的算力和数据，通过 PPO 完成了大模型对齐的首次验证。这一阶段的特点是“重资产”，只有巨头能玩得起。

### 2. 爆发阶段：DPO 的民主化
2023 年底至今，**DPO** 彻底改变了开源社区的格局。
*   **代表模型**：**Zephyr-7B** (HuggingFace)、**Mistral-7B-Instruct** (部分版本)、**OpenHermes**、**Qwen-1.8B**。
*   **发展状态**：DPO 让普通研究者和小团队也能训练出高质量的对齐模型。它极大地降低了对齐的门槛，使得开源模型的能力迅速逼近闭源模型。目前，绝大多数 7B/13B 量级的优秀开源模型都采用了 DPO 或其变体（如 IPO, KTO）。

### 3. 进阶阶段：GRPO 与混合方法的崛起
随着模型规模扩大和任务复杂化，单纯的 DPO 有时难以处理复杂的奖励信号，**GRPO** 等混合方法开始受到国内大厂的青睐。
*   **代表模型**：**Qwen-2 (通义千问 2 代)**、**InternLM-2 (书生·浦语 2 代)**。
*   **发展状态**：国内模型（如阿里、商汤）在发布大参数模型时，往往采用“DPO 打底 + GRPO/PPO 精调”的策略。GRPO 能够更好地利用现有的海量奖励数据和偏好数据，在长对话、工具使用、多模态理解等复杂场景下表现更稳定。

### 总结
*   **PPO** 是**基石**，证明了对齐的可行性。
*   **DPO** 是**革命**，让对齐技术平民化。
*   **GRPO** 是**进化**，代表了工业界处理复杂对齐任务的未来方向。

未来的趋势是**“无模型对齐”**（Model-Free Alignment），即不再依赖显式的 RM 或复杂的 RL 框架，而是通过更巧妙的数学变换（如 DPO 系列）直接从数据中学习偏好。
